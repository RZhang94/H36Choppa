{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"model.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPTEkUPnxsC+/NefuSzNMnC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d7BqCdQuQYB0","executionInfo":{"status":"ok","timestamp":1638586151532,"user_tz":420,"elapsed":13470,"user":{"displayName":"Zhaoxiang Zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10349876649929505294"}},"outputId":"37fc371f-685c-4ecf-a9bf-0b7d3551fc8a"},"source":["# # requirements installation\n","!pip install einops\n","!pip install PyPI\n","!pip install timm"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting einops\n","  Downloading einops-0.3.2-py3-none-any.whl (25 kB)\n","Installing collected packages: einops\n","Successfully installed einops-0.3.2\n","Collecting PyPI\n","  Downloading pypi-2.1.tar.gz (997 bytes)\n","Building wheels for collected packages: PyPI\n","  Building wheel for PyPI (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for PyPI: filename=pypi-2.1-py3-none-any.whl size=1354 sha256=84aff4c153fb9025e8b25348681482e7e390bf2d51733ce06b4a32dbda19c951\n","  Stored in directory: /root/.cache/pip/wheels/9d/58/40/27b525c0b051491cabddd0157355cd3365dfafe2e83618baa6\n","Successfully built PyPI\n","Installing collected packages: PyPI\n","Successfully installed PyPI-2.1\n","Collecting timm\n","  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n","\u001b[K     |████████████████████████████████| 376 kB 4.2 MB/s \n","\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.11.1+cu111)\n","Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.10.0+cu111)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.10.0.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.19.5)\n","Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n","Installing collected packages: timm\n","Successfully installed timm-0.4.12\n"]}]},{"cell_type":"code","metadata":{"id":"VX1BD6JZPU9C","executionInfo":{"status":"ok","timestamp":1638586151533,"user_tz":420,"elapsed":4,"user":{"displayName":"Zhaoxiang Zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10349876649929505294"}}},"source":["import math\n","import logging\n","from functools import partial\n","from collections import OrderedDict\n","from einops import rearrange, repeat      # requires install einops"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"GSoNXNbsRBLb","executionInfo":{"status":"ok","timestamp":1638586156894,"user_tz":420,"elapsed":5363,"user":{"displayName":"Zhaoxiang Zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10349876649929505294"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"xoYTHek6RBEw","executionInfo":{"status":"ok","timestamp":1638586157516,"user_tz":420,"elapsed":628,"user":{"displayName":"Zhaoxiang Zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10349876649929505294"}}},"source":["# timm is a pytorch DL library that can easily create model, load data and so on.\n","from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD   # install timm\n","from timm.models.helpers import load_pretrained\n","from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n","from timm.models.registry import register_model\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"GEq0nsT7fWxq","executionInfo":{"status":"ok","timestamp":1638586157516,"user_tz":420,"elapsed":4,"user":{"displayName":"Zhaoxiang Zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10349876649929505294"}}},"source":["# hidden_feature, in_features, out_features stand for the channel of features.\n","# act_layer = activation layer\n","\n","class Mlp(nn.Module):\n","  def __init__(self, in_features, hidden_features=None, out_features=None, act_layer = nn.GELU, drop=0.):\n","    super().__init__()    # initialize the same way with the nn.module \n","    out_features = out_features or in_features    # ?\n","    hidden_features = hidden_features or in_features # ?\n","\n","    self.fc1 = nn.Linear(in_features, hidden_features)\n","    self.act = act_layer()\n","    self.fc2 = nn.Linear(hidden_features, out_features)\n","    self.drop = nn.Dropout(drop)    # why drop is 0. ?\n","  \n","  def forward(self, x):\n","    # \"\"\"debug\"\"\"\n","    # import pdb\n","    # pdb.set_trace()\n","    x = self.fc1(x)\n","    x = self.act(x)\n","    x = self.drop(x)\n","    x = self.fc2(x)\n","    x = self.drop(x)\n","    return x"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"hJFJB19hfXkh","executionInfo":{"status":"ok","timestamp":1638586157516,"user_tz":420,"elapsed":4,"user":{"displayName":"Zhaoxiang Zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10349876649929505294"}}},"source":["# y = torch.ones(512, 9, 680)\n","# mlp = Mlp(in_features=680, hidden_features=2*680, act_layer=nn.GELU, drop=0.)\n","# y = mlp(y)\n","# y"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gsvx1XBWT7r4","executionInfo":{"status":"ok","timestamp":1638586157517,"user_tz":420,"elapsed":4,"user":{"displayName":"Zhaoxiang Zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10349876649929505294"}}},"source":["class Attention(nn.Module):\n","  def __init__(self,dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.,):\n","    super().__init__()\n","    self.num_heads = num_heads\n","    head_dim = dim // num_heads\n","\n","    self.scale = qk_scale or head_dim ** -0.5   #?\n","    self.qkv = nn.Linear(dim, dim*3, bias=qkv_bias)\n","    self.attn_drop = nn.Dropout(attn_drop)\n","    self.proj = nn.Linear(dim, dim)\n","    self.proj_drop = nn.Dropout(proj_drop)\n","\n","  def forward(self,x):\n","    B, N, C = x.shape\n","    qkv = self.qkv(x).reshape(B,N,3,self.num_heads, C//self.num_heads).permute(2,0,3,1,4)\n","    q, k, v = qkv[0], qkv[1], qkv[2]    # assignt the 3 into q, k, v\n","\n","    #### NOTE: look at the data dimension here\n","    # convert k to transpose and \n","    attn = (q @ k.transpose(-2,-1)) * self.scale    # transpose works the same with permute\n","    # @ compute the inner product of 2 array,\n","    # should look at the q@k, why transpose?\n","\n","    attn = attn.softmax(dim=-1)\n","    attn = self.attn_drop(attn)\n","\n","    x = (attn @ v).transpose(1,2).reshape(B,N,C)\n","    x = self.proj(x)\n","    x = self.proj_drop(x)\n","    return x\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"Njg99koDZcFD","executionInfo":{"status":"ok","timestamp":1638586157517,"user_tz":420,"elapsed":4,"user":{"displayName":"Zhaoxiang Zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10349876649929505294"}}},"source":["import numpy as np\n","# a = np.array([[1,1,1,1],[2,2,2,2]])\n","# b = a\n","# print(a)\n","# a @ b.transpose(-2, -1)\n","# print(b.transpose(-1, -2))\n","# b = b.softmax(dim=-1)\n","# a @ b.transpose()\n","# a = np.ones((4,5))\n","# a.transpose(-2,-1).shape\n","# b = 2*a\n","# a @ b.transpose(-1,-2)\n","# b.transpose(-1,-2)\n","\n","## @ means matrix multiplication"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"rVhpyuTPMBdC","executionInfo":{"status":"ok","timestamp":1638586157795,"user_tz":420,"elapsed":281,"user":{"displayName":"Zhaoxiang Zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10349876649929505294"}}},"source":[""],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"jaU34NrCiWUB","executionInfo":{"status":"ok","timestamp":1638586157796,"user_tz":420,"elapsed":4,"user":{"displayName":"Zhaoxiang Zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10349876649929505294"}}},"source":["class Block(nn.Module):\n","  def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=None, qk_scale=None, drop=0., attn_drop=0., drop_path=0., \n","               act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n","    super().__init__()\n","\n","    self.norm1 = norm_layer(dim)\n","    self.attn = Attention(\n","        dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop\n","    )\n","\n","    ## stochastic depth of drop path\n","    self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n","    self.norm2 = norm_layer(dim)\n","    mlp_hidden_dim = int(dim*mlp_ratio)\n","    self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n","\n","  def forward(self, x):\n","\n","      x = x + self.drop_path(self.attn(self.norm1(x)))\n","      x = x + self.drop_path(self.mlp(self.norm2(x)))\n","      \n","      return x"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"VdRUR32xdgL8","executionInfo":{"status":"ok","timestamp":1638586157796,"user_tz":420,"elapsed":4,"user":{"displayName":"Zhaoxiang Zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10349876649929505294"}}},"source":["class VideoTransformer(nn.Module):\n","  def __init__(self, num_frame=9, in_chans=85, embed_dim_ratio=8, depth=4, \\\n","               num_heads=8, mlp_ratio=2., qkv_bias=True, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\\\n","               drop_path_rate=0.2, norm_layer=None):\n","    super().__init__()\n","\n","    norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)    # partial imported from functional\n","    embed_dim = embed_dim_ratio * in_chans         # 8 * 85             ### the embedded ratio could be adjusted\n","    out_dim = in_chans\n","\n","    ### temporal path embedding\n","    self.Temporal_patch_to_embedding = nn.Linear(in_chans, embed_dim)\n","    self.Temporal_pos_embed = nn.Parameter(torch.zeros(1, num_frame, embed_dim))   ### NOTE: change the embed_dim here\n","    self.pos_drop = nn.Dropout(p=drop_rate)                       ### pos_drop?\n","\n","\n","    dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]      # stochastic depth decay rule\n","\n","    self.blocks = nn.ModuleList([\n","      Block(\n","          dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n","          drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n","      for i in range(depth)\n","    ])\n","\n","    self.Temporal_norm = norm_layer(embed_dim)\n","\n","\n","    # implement weithted mean\n","    self.weighted_mean =torch.nn.Conv1d(in_channels=num_frame, out_channels=1, kernel_size=1)\n","\n","    self.head = nn.Sequential(\n","        nn.LayerNorm(embed_dim),\n","        nn.Linear(embed_dim, out_dim),\n","    )\n","  def Temporal_forward_features(self,x):\n","    # b = x.shape[0]\n","    b, f, p = x.shape\n","\n","    # x = rearrange(x, 'b c f p  -> (b f) p  c', )    #[512, 1, 9, 85]\n","    x = self.Temporal_patch_to_embedding(x)         #[512, 9, 85*8 = 680]\n","\n","    x += self.Temporal_pos_embed      ### NOTE: look after the input dimension\n","    x = self.pos_drop(x)\n","    for blk in self.blocks:\n","      x = blk(x)\n","\n","    x = self.Temporal_norm(x)\n","    # x size [b ,f, emb_dim], then take weighted mean on frame dimension, we only\n","    # predict 3D pose for the central frame\n","    x = self.weighted_mean(x)\n","    x = x.view(b,1,-1)\n","\n","    return x\n","\n","  def forward(self,x):\n","    # x = x.permute(0,3,1,2)\n","    b, f, p = x.shape\n","\n","    x = self.Temporal_forward_features(x)\n","    x = self.head(x)\n","    x = x.view(b,1,p)     # regress to the 85 smpl vector of the central frame\n","\n","    return x\n","\n","\n","\n"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lSrXoeWgPo4U","executionInfo":{"status":"ok","timestamp":1638586337580,"user_tz":420,"elapsed":251,"user":{"displayName":"Zhaoxiang Zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10349876649929505294"}},"outputId":"4bf7f711-c46e-4f45-f66a-837f04320e26"},"source":["# experiment the model by input a tensor with (512, 9, 85)\n","\n","if __name__ == '__main__':\n","  # A = torch.ones(512, 9, 85)\n","  A = torch.ones(3,8,82)\n","  model = VideoTransformer(num_frame=8, in_chans=82, embed_dim_ratio=8, depth=4, \\\n","               num_heads=8, mlp_ratio=2., qkv_bias=True, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\\\n","               drop_path_rate=0.2, norm_layer=None)\n","  # model = VideoTransformer()    # or initialize in this way.\n","  if torch.cuda.is_available():\n","      model = model.cuda()\n","      A = A.cuda()\n","      model.train()\n","      predict = model(A)\n","  else:\n","      model.train()\n","      predict = model(A)    \n","\n","  print(predict.shape)"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 1, 82])\n"]}]},{"cell_type":"code","metadata":{"id":"9wCRTTBabqaV"},"source":["# block = Block(dim=8,num_heads=8)\n","# y = torch.ones(512, 9, 680)\n","# y = block(y)"],"execution_count":null,"outputs":[]}]}